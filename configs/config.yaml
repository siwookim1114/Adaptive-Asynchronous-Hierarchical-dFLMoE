# Adaptive Asynchronous Hierarchical dFLMoE Configuration

# System Configuration
system:
  num_clients: 10
  device: "cuda"  # "cuda" or "cpu"
  seed: 42

# Data Configuration
data:
  dataset: "cifar10"  # "cifar10", "cifar100", "mnist"
  data_dir: "./data"
  partitioning_method: "label_sharding"  # "label_sharding", "dirichlet", "iid"
  classes_per_client: 2  # For label_sharding
  non_iid_alpha: 0.5     # For dirichlet (lower = more heterogeneous)

# Training Configuration
training:
  local_epochs: 3
  batch_size: 64
  learning_rate: 0.01
  momentum: 0.9
  weight_decay: 1e-4
  alpha: 0.3  # MoE weight (0.0 = local only, 1.0 = MoE only)

# Network Configuration
network:
  cluster_size: 5
  top_k_experts: 3
  maintenance_interval: 10  # Epochs between cluster updates
  trust_decay: 0.95

# Model Architecture
model:
  body:
    type: "simple_cnn"  # "simple_cnn" or "resnet18"
    input_channels: 3
    input_size: 32
  
  head:
    input_dim: 512      # Expected input to head from body encoder
    hidden_dim: 256     # Actual hidden layer of the head
    dropout: 0.1
  
  router:
    input_dim: 512      # ‚Üê Must match body.feature_dim
    hidden_dim: 256
    temperature: 1.0
  
  fst:
    enabled: false
    transform_dim: 512

# Evaluation Configuration
evaluation:
  eval_interval: 50  # Steps between evaluations
  test_batch_size: 256